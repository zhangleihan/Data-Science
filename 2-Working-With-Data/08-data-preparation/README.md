<!-- # Working with Data: Data Preparation -->
# æ•°æ®é¢„å¤„ç†

|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/08-DataPreparation.png)|
|:---:|
|Data Preparation - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |

<!-- ## [Pre-Lecture Quiz](https://purple-hill-04aebfb03.1.azurestaticapps.net/quiz/14) -->



Depending on its source, raw data may contain some inconsistencies that will cause challenges in analysis and modeling. In other words, this data can be categorized as â€œdirtyâ€ and will need to be cleaned up. This lesson focuses on techniques for cleaning and transforming the data to handle challenges of missing, inaccurate, or incomplete data. Topics covered in this lesson will utilize Python and the Pandas library and will be [demonstrated in the notebook](notebook.ipynb) within this directory.

æ ¹æ®å…¶æ¥æºï¼ŒåŸå§‹æ•°æ®å¯èƒ½åŒ…å«ä¸€äº›ä¸ä¸€è‡´ä¹‹å¤„ï¼Œè¿™å°†ç»™åˆ†æå’Œå»ºæ¨¡å¸¦æ¥æŒ‘æˆ˜ã€‚æ¢å¥è¯è¯´ï¼Œè¿™äº›æ•°æ®å¯ä»¥è¢«å½’ç±»ä¸ºâ€œè„â€æ•°æ®ï¼Œéœ€è¦æ¸…ç†ã€‚æœ¬è¯¾é‡ç‚¹ä»‹ç»æ¸…ç†å’Œè½¬æ¢æ•°æ®çš„æŠ€æœ¯ï¼Œä»¥åº”å¯¹æ•°æ®ç¼ºå¤±ã€ä¸å‡†ç¡®æˆ–ä¸å®Œæ•´çš„æŒ‘æˆ˜ã€‚åˆ©ç”¨ Python å’Œ Pandas åº“è¿›è¡Œæ•°æ®é¢„å¤„ç†çš„ç¨‹åºè§æœ¬ç›®å½•ä¸­çš„[notebook.ipynb](notebook.ipynb)ã€‚

<!-- ## The importance of cleaning data -->
## æ¸…æ´—æ•°æ®çš„é‡è¦æ€§

<!-- - **Ease of use and reuse**: When data is properly organized and normalized itâ€™s easier to search, use, and share with others.

- **Consistency**: Data science often requires working with more than one dataset, where datasets from different sources need to be joined together. Making sure that each individual data set has common standardization will ensure that the data is still useful when they are all merged into one dataset.

- **Model accuracy**: Data that has been cleaned improves the accuracy of models that rely on it. -->

- **æ˜“äºä½¿ç”¨å’Œé‡ç”¨**ï¼šå½“æ•°æ®è¢«æ­£ç¡®ç»„ç»‡å’Œè§„èŒƒåŒ–æ—¶ï¼Œå®ƒæ›´å®¹æ˜“æœç´¢ã€ä½¿ç”¨å’Œä¸ä»–äººå…±äº«ã€‚

- **ä¸€è‡´æ€§**ï¼šæ•°æ®ç§‘å­¦é€šå¸¸éœ€è¦å¤„ç†å¤šä¸ªæ•°æ®é›†ï¼Œå…¶ä¸­æ¥è‡ªä¸åŒæ¥æºçš„æ•°æ®é›†éœ€è¦è¿æ¥åœ¨ä¸€èµ·ã€‚ç¡®ä¿æ¯ä¸ªå•ç‹¬çš„æ•°æ®é›†å…·æœ‰å…±åŒçš„æ ‡å‡†åŒ–å°†ç¡®ä¿å½“å®ƒä»¬å…¨éƒ¨åˆå¹¶ä¸ºä¸€ä¸ªæ•°æ®é›†æ—¶æ•°æ®ä»ç„¶æœ‰ç”¨ã€‚

- **æ¨¡å‹å‡†ç¡®æ€§**ï¼šæ¸…ç†åçš„æ•°æ®å¯ä»¥æé«˜ä¾èµ–äºå®ƒçš„æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚

<!-- ## Common cleaning goals and strategies -->
## æ•°æ®æ¸…æ´—çš„ç›®æ ‡å’Œç­–ç•¥

- **æ¢ç´¢æ•°æ®é›†**ï¼šæ•°æ®æ¢ç´¢ï¼ˆå°†åœ¨åé¢çš„è¯¾ç¨‹ä¸­ä»‹ç»ï¼‰å¯ä»¥å¸®åŠ©æ‚¨å‘ç°éœ€è¦æ¸…ç†çš„æ•°æ®ã€‚ç›´è§‚åœ°è§‚å¯Ÿæ•°æ®é›†ä¸­çš„å€¼å¯ä»¥è®¾å®šå…¶ä½™æ•°æ®çš„é¢„æœŸï¼Œæˆ–æä¾›å¯ä»¥è§£å†³çš„é—®é¢˜çš„æƒ³æ³•ã€‚æ¢ç´¢å¯èƒ½æ¶‰åŠåŸºæœ¬æŸ¥è¯¢ã€å¯è§†åŒ–å’Œé‡‡æ ·ã€‚

- **æ ¼å¼**ï¼šæ ¹æ®æ¥æºçš„ä¸åŒï¼Œæ•°æ®çš„å‘ˆç°æ–¹å¼å¯èƒ½ä¸ä¸€è‡´ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´åœ¨æœç´¢å’Œè¡¨ç¤ºå€¼æ—¶å‡ºç°é—®é¢˜ï¼Œå³åœ¨æ•°æ®é›†ä¸­å¯ä»¥çœ‹åˆ°å€¼ï¼Œä½†åœ¨å¯è§†åŒ–æˆ–æŸ¥è¯¢ç»“æœä¸­æ— æ³•æ­£ç¡®è¡¨ç¤ºã€‚å¸¸è§çš„æ ¼å¼é—®é¢˜åŒ…æ‹¬è§£å†³ç©ºæ ¼ã€æ—¥æœŸå’Œæ•°æ®ç±»å‹ã€‚è§£å†³æ ¼å¼é—®é¢˜é€šå¸¸ç”±ä½¿ç”¨æ•°æ®çš„äººå†³å®šã€‚ä¾‹å¦‚ï¼Œæ—¥æœŸå’Œæ•°å­—çš„å‘ˆç°æ ‡å‡†å¯èƒ½å› å›½å®¶/åœ°åŒºè€Œå¼‚ã€‚

- **å»é™¤é‡å¤**ï¼šå‡ºç°å¤šæ¬¡çš„æ•°æ®ä¼šäº§ç”Ÿä¸å‡†ç¡®çš„ç»“æœï¼Œé€šå¸¸åº”å°†å…¶åˆ é™¤ã€‚å°†ä¸¤ä¸ªæˆ–å¤šä¸ªæ•°æ®é›†è¿æ¥åœ¨ä¸€èµ·æ—¶ï¼Œè¿™ç§æƒ…å†µå¾ˆå¸¸è§ã€‚ä½†æ˜¯ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿æ¥æ•°æ®é›†ä¸­çš„é‡å¤é¡¹å¯èƒ½åŒ…å«å¯æä¾›æ›´å¤šä¿¡æ¯çš„éƒ¨åˆ†ï¼Œå¯èƒ½éœ€è¦ä¿ç•™ã€‚

- **ç¼ºå¤±æ•°æ®**ï¼šç¼ºå¤±æ•°æ®å¯èƒ½å¯¼è‡´ç»“æœä¸å‡†ç¡®ä»¥åŠç»“æœä¸å‡†ç¡®æˆ–å­˜åœ¨åå·®ã€‚æœ‰æ—¶å¯ä»¥é€šè¿‡â€œé‡æ–°åŠ è½½â€æ•°æ®ã€ä½¿ç”¨è®¡ç®—å’Œ Python ç­‰ä»£ç å¡«å……ç¼ºå¤±å€¼æˆ–ç®€å•åœ°åˆ é™¤å€¼å’Œç›¸åº”æ•°æ®æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æ•°æ®ç¼ºå¤±çš„åŸå› æœ‰å¾ˆå¤šï¼Œè§£å†³è¿™äº›ç¼ºå¤±å€¼æ‰€é‡‡å–çš„æªæ–½å¯èƒ½å–å†³äºå®ƒä»¬æœ€åˆæ˜¯å¦‚ä½•ä»¥åŠä¸ºä½•ç¼ºå¤±çš„ã€‚

<!-- - **Exploring a dataset**: Data exploration, which is covered in a [later lesson](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/4-Data-Science-Lifecycle/15-analyzing) can help you discover data that needs to be cleaned up. Visually observing values within a dataset can set expectations of what that rest of it will look like, or provide an idea of the problems that can be resolved. Exploration can involve basic querying, visualizations, and sampling.

-  **Formatting**: Depending on the source, data can have inconsistencies in how itâ€™s presented. This can cause problems in searching for and representing the value, where itâ€™s seen within the dataset but is not properly represented in visualizations or query results. Common formatting problems involve resolving whitespace, dates, and data types. Resolving formatting issues is typically up to the people who are using the data. For example, standards on how dates and numbers are presented can differ by country. 

-  **Duplications**: Data that has more than one occurrence can produce inaccurate results and usually should be removed. This can be a common occurrence when joining two or more datasets together. However, there are instances where duplication in joined datasets contain pieces that can provide additional information and may need to be preserved.

- **Missing Data**: Missing data can cause inaccuracies as well as weak or biased results. Sometimes these can be resolved by a "reload" of the data, filling in the missing values with computation and code like Python, or simply just removing the value and corresponding data. There are numerous reasons for why data may be missing and the actions that are taken to resolve these missing values can be dependent on how and why they went missing in the first place.  -->

<!-- ## Exploring DataFrame information -->
## æ¢ç´¢DataFrameä¿¡æ¯

<!-- > **Learning goal:** By the end of this subsection, you should be comfortable finding general information about the data stored in pandas DataFrames. -->

 **å­¦ä¹ ç›®æ ‡**ï¼šåœ¨æœ¬å°èŠ‚ç»“æŸæ—¶ï¼Œèƒ½å¤Ÿè½»æ¾æ‰¾åˆ°æœ‰å…³å­˜å‚¨åœ¨ pandas DataFrames ä¸­çš„æ•°æ®çš„ç›®æ ‡ä¿¡æ¯ã€‚

<!-- Once you have loaded your data into pandas, it will more likely than not be in a DataFrame(refer to the previous [lesson](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/2-Working-With-Data/07-python#dataframe) for detailed overview). However, if the data set in your DataFrame has 60,000 rows and 400 columns, how do you even begin to get a sense of what you're working with? Fortunately, [pandas](https://pandas.pydata.org/) provides some convenient tools to quickly look at overall information about a DataFrame in addition to the first few and last few rows. -->

å°†æ•°æ®åŠ è½½åˆ° pandas ä¸­åï¼Œæ•°æ®å¾ˆå¯èƒ½ä»¥ DataFrame çš„å½¢å¼å‘ˆç°ã€‚ä½†æ˜¯ï¼Œå¦‚æœ DataFrame ä¸­çš„æ•°æ®é›†æœ‰ 60,000 è¡Œå’Œ 400 åˆ—ï¼Œè¯¥å¦‚ä½•å¼€å§‹äº†è§£æ­£åœ¨å¤„ç†çš„å†…å®¹ï¼Ÿå¹¸è¿çš„æ˜¯ï¼Œé™¤äº†å‰å‡ è¡Œå’Œåå‡ â€‹â€‹è¡Œä¹‹å¤–ï¼Œ pandasè¿˜æä¾›äº†ä¸€äº›ä¾¿æ·çš„å·¥å…·ï¼Œå¯ä»¥å¿«é€ŸæŸ¥çœ‹ DataFrame çš„æ•´ä½“ä¿¡æ¯ã€‚

<!-- In order to explore this functionality, we will import the Python scikit-learn library and use an iconic dataset: the **Iris data set**. -->
ä¸ºäº†æ¢ç´¢æ­¤åŠŸèƒ½ï¼Œæˆ‘ä»¬å°†å¯¼å…¥ Python scikit-learn åº“å¹¶ä½¿ç”¨æ ‡å¿—æ€§æ•°æ®é›†ï¼š**Iris æ•°æ®é›†**ã€‚

```python
import pandas as pd
from sklearn.datasets import load_iris

iris = load_iris()
iris_df = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])
```
|                                        |sepal lengthè¼ç‰‡é•¿ (cm)|sepal widthè¼ç‰‡å®½ (cm)|petal lengthèŠ±ç“£é•¿ (cm)|petal widthèŠ±ç“£æ¬¾ (cm)|
|----------------------------------------|-----------------|----------------|-----------------|----------------|
|0                                       |5.1              |3.5             |1.4              |0.2             |
|1                                       |4.9              |3.0             |1.4              |0.2             |
|2                                       |4.7              |3.2             |1.3              |0.2             |
|3                                       |4.6              |3.1             |1.5              |0.2             |
|4                                       |5.0              |3.6             |1.4              |0.2             |

<!-- - **DataFrame.info**: To start off, the `info()` method is used to print a summary of the content present in a `DataFrame`. Let's take a look at this dataset to see what we have: -->
- **DataFrame.info**: é¦–å…ˆï¼Œè¯¥info()æ–¹æ³•ç”¨äºæ‰“å° ä¸­å­˜åœ¨çš„å†…å®¹æ‘˜è¦DataFrameã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹è¿™ä¸ªæ•°æ®é›†ï¼Œçœ‹çœ‹æˆ‘ä»¬æœ‰ä»€ä¹ˆï¼š

```python
iris_df.info()
```
```
RangeIndex: 150 entries, 0 to 149
Data columns (total 4 columns):
 #   Column             Non-Null Count  Dtype  
---  ------             --------------  -----  
 0   sepal length (cm)  150 non-null    float64
 1   sepal width (cm)   150 non-null    float64
 2   petal length (cm)  150 non-null    float64
 3   petal width (cm)   150 non-null    float64
dtypes: float64(4)
memory usage: 4.8 KB
```
<!-- From this, we know that the *Iris* dataset has 150 entries in four columns with no null entries. All of the data is stored as 64-bit floating-point numbers. -->

- **DataFrame.head()**: æ£€æŸ¥æ•°æ®é›†å‰å‡ è¡Œï¼š
<!-- Next, to check the actual content of the `DataFrame`, we use the `head()` method. Let's see what the first few rows of our `iris_df` look like: -->
```python
iris_df.head()
```
```
   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)
0                5.1               3.5                1.4               0.2
1                4.9               3.0                1.4               0.2
2                4.7               3.2                1.3               0.2
3                4.6               3.1                1.5               0.2
4                5.0               3.6                1.4               0.2
```
- **DataFrame.tail()**: æ£€æŸ¥æ•°æ®é›†æœ€åå‡ è¡Œï¼š
<!-- Conversely, to check the last few rows of the `DataFrame`, we use the `tail()` method: -->
```python
iris_df.tail()
```
```
     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)
145                6.7               3.0                5.2               2.3
146                6.3               2.5                5.0               1.9
147                6.5               3.0                5.2               2.0
148                6.2               3.4                5.4               2.3
149                5.9               3.0                5.1               1.8
```
<!-- > **Takeaway:** Even just by looking at the metadata about the information in a DataFrame or the first and last few values in one, you can get an immediate idea about the size, shape, and content of the data you are dealing with. -->
> å³ä½¿ä»…ä»…é€šè¿‡æŸ¥çœ‹ DataFrame ä¸­ä¿¡æ¯çš„å…ƒæ•°æ®æˆ–å…¶ä¸­çš„å‰å‡ ä¸ªå’Œåå‡ ä¸ªå€¼ï¼Œä¹Ÿå¯ä»¥ç«‹å³äº†è§£æ­£åœ¨å¤„ç†çš„æ•°æ®çš„å¤§å°ã€ç»´åº¦å’Œå†…å®¹ã€‚

<!-- ## Dealing with Missing Data -->
## å¤„ç†ç¼ºå¤±æ•°æ®
<!-- > **Learning goal:** By the end of this subsection, you should know how to replace or remove null values from DataFrames. -->
> å­¦ä¹ ç›®æ ‡ï¼šçŸ¥é“å¦‚ä½•ä» DataFrames ä¸­æ›¿æ¢æˆ–åˆ é™¤ç©ºå€¼

<!-- Most of the time the datasets you want to use (of have to use) have missing values in them. How missing data is handled carries with it subtle tradeoffs that can affect your final analysis and real-world outcomes. -->

<!-- Pandas handles missing values in two ways. The first you've seen before in previous sections: `NaN`, or Not a Number. This is a actually a special value that is part of the IEEE floating-point specification and it is only used to indicate missing floating-point values. -->

<!-- For missing values apart from floats, pandas uses the Python `None` object. While it might seem confusing that you will encounter two different kinds of values that say essentially the same thing, there are sound programmatic reasons for this design choice and, in practice, going this route enables pandas to deliver a good compromise for the vast majority of cases. Notwithstanding this, both `None` and `NaN` carry restrictions that you need to be mindful of with regards to how they can be used. -->

å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæƒ³è¦ä½¿ç”¨çš„æ•°æ®é›†ä¸­éƒ½æœ‰ç¼ºå¤±å€¼ã€‚å¦‚ä½•å¤„ç†ç¼ºå¤±æ•°æ®ä¼šå¸¦æ¥å¾®å¦™çš„å½±å“ï¼Œè¿™å¯èƒ½ä¼šå½±å“æ‚¨çš„æœ€ç»ˆåˆ†æå’Œå®é™…ç»“æœã€‚

Pandas ä»¥ä¸¤ç§æ–¹å¼å¤„ç†ç¼ºå¤±å€¼ã€‚ç¬¬ä¸€ç§æ–¹å¼åœ¨å‰é¢çš„éƒ¨åˆ†ä¸­å·²ç»è§è¿‡ï¼š`NaN`ï¼Œæˆ–éæ•°å­—ã€‚è¿™å®é™…ä¸Šæ˜¯ IEEE æµ®ç‚¹è§„èŒƒçš„ä¸€éƒ¨åˆ†ï¼Œå®ƒä»…ç”¨äºæŒ‡ç¤ºç¼ºå¤±çš„æµ®ç‚¹å€¼ã€‚

å¯¹äºé™¤æµ®ç‚¹æ•°ä¹‹å¤–çš„ç¼ºå¤±å€¼ï¼Œpandas ä½¿ç”¨ Pythonçš„`None`å¯¹è±¡ã€‚è™½ç„¶å¯èƒ½ä¼šé‡åˆ°ä»¤äººå›°æƒ‘çš„ä¸¤ç§è¡¨ç¤ºç›¸åŒå†…å®¹çš„ä¸åŒå€¼ï¼Œä½†è¿™ç§è®¾è®¡é€‰æ‹©æœ‰åˆç†çš„ç¼–ç¨‹åŸå› ï¼Œå¹¶ä¸”åœ¨å®è·µä¸­ï¼Œé‡‡ç”¨è¿™ç§æ–¹å¼ä½¿ pandas èƒ½å¤Ÿåœ¨ç»å¤§å¤šæ•°æƒ…å†µä¸‹æä¾›è‰¯å¥½çš„æŠ˜è¡·æ–¹æ¡ˆã€‚å°½ç®¡å¦‚æ­¤ï¼Œ`None`å’Œ`NaN`éƒ½æœ‰ä½¿ç”¨é™åˆ¶ã€‚

<!-- Check out more about `NaN` and `None` from the [notebook](https://github.com/microsoft/Data-Science-For-Beginners/blob/main/4-Data-Science-Lifecycle/15-analyzing/notebook.ipynb)! -->
å¯ä»¥ä»è¯¥ä»£ç [notebook](https://github.com/microsoft/Data-Science-For-Beginners/blob/main/4-Data-Science-Lifecycle/15-analyzing/notebook.ipynb)ä¸­äº†è§£æ›´å¤šæœ‰å…³çš„`NaN`å’Œ`None`çš„ä½¿ç”¨æ–¹æ³•ã€‚

- **Detecting null values**: In `pandas`, the `isnull()` and `notnull()` methods are your primary methods for detecting null data. Both return Boolean masks over your data. We will be using `numpy` for `NaN` values:
- æ£€æµ‹ç©ºå€¼ï¼šåœ¨`pandas`ä¸­ï¼Œ`isnull()`å’Œ`notnull()`æ–¹æ³•æ˜¯æ£€æµ‹ç©ºæ•°æ®çš„ä¸»è¦æ–¹æ³•ã€‚ä¸¤è€…éƒ½ä¼šè¿”å›æ•°æ®çš„å¸ƒå°”å€¼ã€‚æˆ‘ä»¬å°†ä½¿ç”¨`numpy`å€¼`NaN`ï¼š
```python
import numpy as np

example1 = pd.Series([0, np.nan, '', None])
example1.isnull()
```
```
0    False
1     True
2    False
3     True
dtype: bool
```
Look closely at the output. Does any of it surprise you? While `0` is an arithmetic null, it's nevertheless a perfectly good integer and pandas treats it as such. `''` is a little more subtle. While we used it in Section 1 to represent an empty string value, it is nevertheless a string object and not a representation of null as far as pandas is concerned.

Now, let's turn this around and use these methods in a manner more like you will use them in practice. You can use Boolean masks  directly as a ``Series`` or ``DataFrame`` index, which can be useful when trying to work with isolated missing (or present) values.

> **Takeaway**: Both the `isnull()` and `notnull()` methods produce similar results when you use them in `DataFrame`s: they show the results and the index of those results, which will help you enormously as you wrestle with your data.

<!-- - **Dropping null values**: Beyond identifying missing values, pandas provides a convenient means to remove null values from `Series` and `DataFrame`s. (Particularly on large data sets, it is often more advisable to simply remove missing [NA] values from your analysis than deal with them in other ways.) To see this in action, let's return to `example1`: -->

- **ä¸¢å¼ƒç©ºæ•°æ®
é™¤äº†æ£€æµ‹ç¼ºå¤±æ•°æ®ï¼Œpandasæä¾›äº†åœ¨Serieså’ŒDataFrameæ£€æµ‹ç©ºæ•°æ®çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§æ•°æ®é›†ï¼Œé€šå¸¸éƒ½ä¼šå­˜åœ¨ç©ºæ•°æ®çš„æƒ…å†µã€‚ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä¸¢å¼ƒç©ºæ•°æ®ï¼š
```python
example1 = example1.dropna()
example1
```
```
0    0
2     
dtype: object
```
Note that this should look like your output from `example3[example3.notnull()]`. The difference here is that, rather than just indexing on the masked values, `dropna` has removed those missing values from the `Series` `example1`.

Because `DataFrame`s have two dimensions, they afford more options for dropping data.

```python
example2 = pd.DataFrame([[1,      np.nan, 7], 
                         [2,      5,      8], 
                         [np.nan, 6,      9]])
example2
```
|      | 0 | 1 | 2 |
|------|---|---|---|
|0     |1.0|NaN|7  |
|1     |2.0|5.0|8  |
|2     |NaN|6.0|9  |

<!-- (Did you notice that pandas upcast two of the columns to floats to accommodate the `NaN`s?) -->
ï¼ˆæ˜¯å¦å‘ç°äº†pandasä¸ºäº†åŒ¹é…`NaN`æ•°å€¼ï¼Œå°†ä¸¤åˆ—ç‰¹å¾å€¼è½¬æ¢æˆäº†æµ®ç‚¹æ•°ï¼‰

<!-- You cannot drop a single value from a `DataFrame`, so you have to drop full rows or columns. Depending on what you are doing, you might want to do one or the other, and so pandas gives you options for both. Because in data science, columns generally represent variables and rows represent observations, you are more likely to drop rows of data; the default setting for `dropna()` is to drop all rows that contain any null values: -->
ä¸èƒ½ä»`DataFrame`ä¸­åˆ é™¤å•ä¸ªå€¼ï¼Œå› æ­¤å¿…é¡»åˆ é™¤æ•´è¡Œæˆ–æ•´åˆ—ã€‚æ ¹æ®æ­£åœ¨åšçš„åˆ†æï¼Œå¯èƒ½æƒ³è¦æ‰§è¡Œå…¶ä¸­ä¸€ä¸ªæˆ–å¤šä¸ªæ“ä½œï¼Œpandasæä¾›äº†ä¸¤ç§é€‰é¡¹ã€‚å› ä¸ºåœ¨æ•°æ®ç§‘å­¦ä¸­ï¼Œåˆ—é€šå¸¸è¡¨ç¤ºå˜é‡ï¼Œè¡Œè¡¨ç¤ºè§‚å¯Ÿå€¼ï¼ˆæ ·æœ¬æ•°æ®ï¼‰ï¼Œæ‰€ä»¥æ›´æœ‰å¯èƒ½åˆ é™¤æ•°æ®è¡Œï¼›dropna()é»˜è®¤æ˜¯åˆ é™¤åŒ…å«ä»»ä½•ç©ºå€¼çš„æ‰€æœ‰è¡Œï¼š

```python
example2.dropna()
```
```
	0	1	2
1	2.0	5.0	8
```
If necessary, you can drop NA values from columns. Use `axis=1` to do so:
```python
example2.dropna(axis='columns')
```
```
	2
0	7
1	8
2	9
```
<!-- Notice that this can drop a lot of data that you might want to keep, particularly in smaller datasets. What if you just want to drop rows or columns that contain several or even just all null values? You specify those setting in `dropna` with the `how` and `thresh` parameters. -->
è¯·æ³¨æ„ï¼Œè¿™å¯èƒ½ä¼šåˆ é™¤æƒ³è¦ä¿ç•™çš„å¤§é‡æ•°æ®ï¼Œå°¤å…¶æ˜¯åœ¨è¾ƒå°çš„æ•°æ®é›†ä¸­ã€‚å¦‚æœåªæƒ³åˆ é™¤åŒ…å«å¤šä¸ªç”šè‡³æ‰€æœ‰ç©ºå€¼çš„è¡Œæˆ–åˆ—ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿå¯ä»¥é€šè¿‡è®¾ç½®howå’Œthreshä¸¤ä¸ªå‚æ•°ä½¿ç”¨dropnaæ–¹æ³•ã€‚

<!-- By default, `how='any'` (if you would like to check for yourself or see what other parameters the method has, run `example4.dropna?` in a code cell). You could alternatively specify `how='all'` so as to drop only rows or columns that contain all null values. Let's expand our example `DataFrame` to see this in action. -->
é»˜è®¤æƒ…å†µä¸‹`how='any'`ï¼ˆå¦‚æœæƒ³è‡ªå·±æ£€æŸ¥æˆ–æŸ¥çœ‹è¯¥æ–¹æ³•è¿˜æœ‰å“ªäº›å…¶ä»–å‚æ•°ï¼Œå¯ä»¥è¿è¡Œ`example4.dropna?`ï¼‰ã€‚ä¹Ÿå¯ä»¥æŒ‡å®š`how='all'`ï¼Œè¯¥æ“ä½œä»…åˆ é™¤åŒ…å«æ‰€æœ‰ç©ºå€¼çš„è¡Œæˆ–åˆ—ã€‚

```python
example2[3] = np.nan
example2
```
|      |0  |1  |2  |3  |
|------|---|---|---|---|
|0     |1.0|NaN|7  |NaN|
|1     |2.0|5.0|8  |NaN|
|2     |NaN|6.0|9  |NaN|

The `thresh` parameter gives you finer-grained control: you set the number of *non-null* values that a row or column needs to have in order to be kept:
```python
example2.dropna(axis='rows', thresh=3)
```
```
	0	1	2	3
1	2.0	5.0	8	NaN
```
Here, the first and last row have been dropped, because they contain only two non-null values.

<!-- - **Filling null values**: Depending on your dataset, it can sometimes make more sense to fill null values with valid ones rather than drop them. You could use `isnull` to do this in place, but that can be laborious, particularly if you have a lot of values to fill. Because this is such a common task in data science, pandas provides `fillna`, which returns a copy of the `Series` or `DataFrame` with the missing values replaced with one of your choosing. Let's create another example `Series` to see how this works in practice. -->
å¡«å……ç©ºå€¼ï¼šæ ¹æ®æ•°æ®é›†ï¼Œæœ‰æ—¶ç”¨æœ‰æ•ˆå€¼å¡«å……ç©ºå€¼æ¯”åˆ é™¤å®ƒä»¬æ›´æœ‰æ„ä¹‰ã€‚å¯ä»¥ä½¿ç”¨`isnull`æ¥åˆ¤å®šå¹¶æ‰§è¡Œæ­¤æ“ä½œï¼Œä½†è¿™å¯èƒ½å¾ˆè´¹åŠ›ï¼Œç‰¹åˆ«æ˜¯å¦‚æœæœ‰å¾ˆå¤šå€¼éœ€è¦å¡«å……ã€‚ç”±äºè¿™æ˜¯æ•°æ®ç§‘å­¦ä¸­éå¸¸å¸¸è§çš„ä»»åŠ¡ï¼Œå› æ­¤pandasæä¾›äº†`fillna`æ–¹æ³•ï¼Œå®ƒè¿”å›DataFrameçš„å‰¯æœ¬ï¼Œå…¶ä¸­ç¼ºå¤±å€¼æ›¿æ¢ä¸ºæŒ‡å®šçš„å€¼ã€‚

```python
example3 = pd.Series([1, np.nan, 2, None, 3], index=list('abcde'))
example3
```
```
a    1.0
b    NaN
c    2.0
d    NaN
e    3.0
dtype: float64
```
You can fill all of the null entries with a single value, such as `0`:
```python
example3.fillna(0)
```
```
a    1.0
b    0.0
c    2.0
d    0.0
e    3.0
dtype: float64
```
You can **forward-fill** null values, which is to use the last valid value to fill a null:
```python
example3.fillna(method='ffill')
```
```
a    1.0
b    1.0
c    2.0
d    2.0
e    3.0
dtype: float64
```
You can also **back-fill** to propagate the next valid value backward to fill a null:
```python
example3.fillna(method='bfill')
```
```
a    1.0
b    2.0
c    2.0
d    3.0
e    3.0
dtype: float64
```
As you might guess, this works the same with `DataFrame`s, but you can also specify an `axis` along which to fill null values. taking the previously used `example2` again:
```python
example2.fillna(method='ffill', axis=1)
```
```
	0	1	2	3
0	1.0	1.0	7.0	7.0
1	2.0	5.0	8.0	8.0
2	NaN	6.0	9.0	9.0
```
<!-- Notice that when a previous value is not available for forward-filling, the null value remains. -->
è¯·æ³¨æ„ï¼Œå½“å‰ä¸€ä¸ªå€¼ä¸å¯ç”¨äºå‰å‘å¡«å……æ—¶ï¼Œå°†ä¿ç•™ç©ºå€¼ã€‚

<!-- > **Takeaway:** There are multiple ways to deal with missing values in your datasets. The specific strategy you use (removing them, replacing them, or even how you replace them) should be dictated by the particulars of that data. You will develop a better sense of how to deal with missing values the more you handle and interact with datasets. -->
> **è¦ç‚¹**ï¼šæœ‰å¤šç§æ–¹æ³•å¯ä»¥å¤„ç†æ•°æ®é›†ä¸­çš„ç¼ºå¤±å€¼ã€‚ä½¿ç”¨çš„å…·ä½“ç­–ç•¥ï¼ˆåˆ é™¤å®ƒä»¬ã€æ›¿æ¢å®ƒä»¬ï¼Œç”šè‡³æ›¿æ¢å®ƒä»¬çš„æ–¹å¼ï¼‰åº”ç”±æ•°æ®çš„å…·ä½“æƒ…å†µå†³å®šã€‚å¤„ç†å’Œä¸æ•°æ®é›†äº¤äº’çš„æ¬¡æ•°è¶Šå¤šï¼Œå°±ä¼šå¯¹å¦‚ä½•å¤„ç†ç¼ºå¤±å€¼æœ‰æ›´å¥½çš„ç†è§£ã€‚

<!-- ## Removing duplicate data -->
## å»é‡å……åˆ†æ•°æ®

<!-- > **Learning goal:** By the end of this subsection, you should be comfortable identifying and removing duplicate values from DataFrames. -->

> å­¦ä¹ ç›®æ ‡ï¼šèƒ½å¤Ÿè½»æ¾åœ°è¯†åˆ«å’Œåˆ é™¤DataFramesä¸­çš„é‡å¤å€¼ã€‚

<!-- In addition to missing data, you will often encounter duplicated data in real-world datasets. Fortunately, `pandas` provides an easy means of detecting and removing duplicate entries. -->

é™¤äº†ç¼ºå¤±æ•°æ®å¤–ï¼Œè¿˜ç»å¸¸ä¼šåœ¨å®é™…æ•°æ®é›†ä¸­é‡åˆ°é‡å¤æ•°æ®ã€‚å¹¸è¿çš„æ˜¯ï¼Œ`pandas`æä¾›äº†ä¸€ç§æ£€æµ‹å’Œåˆ é™¤é‡å¤æ¡ç›®çš„ç®€å•æ–¹æ³•ã€‚

- **Identifying duplicates: `duplicated`**: You can easily spot duplicate values using the `duplicated` method in pandas, which returns a Boolean mask indicating whether an entry in a `DataFrame` is a duplicate of an earlier one. Let's create another example `DataFrame` to see this in action.

- è¯†åˆ«é‡å¤é¡¹ï¼šå¯ä»¥ä½¿ç”¨ pandas ä¸­çš„æ–¹æ³•`duplicated`è½»æ¾å‘ç°é‡å¤å€¼ï¼Œè¯¥æ–¹æ³•è¿”å›ä¸€ä¸ªå¸ƒå°”æ©ç ï¼ŒæŒ‡ç¤º`DataFrame`ä¸­çš„æ•°æ®è®°å½•æ˜¯å¦æ˜¯å…ˆå‰æ¡ç›®çš„é‡å¤é¡¹ã€‚

```python
example4 = pd.DataFrame({'letters': ['A','B'] * 2 + ['B'],
                         'numbers': [1, 2, 1, 3, 3]})
example4
```
|      |letters|numbers|
|------|-------|-------|
|0     |A      |1      |
|1     |B      |2      |
|2     |A      |1      |
|3     |B      |3      |
|4     |B      |3      |

```python
example4.duplicated()
```
```
0    False
1    False
2     True
3    False
4     True
dtype: bool
```
<!-- - **Dropping duplicates: `drop_duplicates`:** simply returns a copy of the data for which all of the `duplicated` values are `False`: -->

- **åˆ é™¤é‡å¤é¡¹**ï¼š`drop_duplicates`ä»…è¿”å›`duplicated`çš„å€¼æ˜¯`False`çš„æ•°æ®çš„å‰¯æœ¬ï¼š

```python
example4.drop_duplicates()
```
```
	letters	numbers
0	A	1
1	B	2
3	B	3
```
Both `duplicated` and `drop_duplicates` default to consider all columns but you can specify that they examine only a subset of columns in your `DataFrame`:
```python
example4.drop_duplicates(['letters'])
```
```
letters	numbers
0	A	1
1	B	2
```

<!-- > **Takeaway:** Removing duplicate data is an essential part of almost every data-science project. Duplicate data can change the results of your analyses and give you inaccurate results! -->

> è¦ç‚¹ï¼šåˆ é™¤é‡å¤æ•°æ®æ˜¯å‡ ä¹æ¯ä¸ªæ•°æ®ç§‘å­¦é¡¹ç›®çš„é‡è¦éƒ¨åˆ†ã€‚é‡å¤æ•°æ®å¯èƒ½ä¼šæ”¹å˜åˆ†æç»“æœå¹¶å¯¼è‡´ä¸å‡†ç¡®çš„ç»“æœï¼


<!-- ## ğŸš€ Challenge

All of the discussed materials are provided as a [Jupyter Notebook](https://github.com/microsoft/Data-Science-For-Beginners/blob/main/2-Working-With-Data/08-data-preparation/notebook.ipynb). Additionally, there are exercises present after each section, give them a try! -->

<!-- ## [Post-Lecture Quiz](https://purple-hill-04aebfb03.1.azurestaticapps.net/quiz/15) -->



<!-- ## Review & Self Study

There are many ways to discover and approach preparing your data for analysis and modeling and cleaning the data is an important step that is a "hands on" experience. Try these challenges from Kaggle to explore techniques that this lesson didn't cover.

- [Data Cleaning Challenge: Parsing Dates](https://www.kaggle.com/rtatman/data-cleaning-challenge-parsing-dates/)

- [Data Cleaning Challenge: Scale and Normalize Data](https://www.kaggle.com/rtatman/data-cleaning-challenge-scale-and-normalize-data) -->


<!-- ## Assignment -->
## ä½œä¸š

[Evaluating Data from a Form](assignment.md)
